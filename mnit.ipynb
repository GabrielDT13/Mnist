{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximo valor de la imagen: tensor(1.)\n",
      "Mínimo valor de la imagen: tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "# Definimos las transformaciones para normalizar los datos\n",
    "# Las imágenes originales están en escala de grises con valores entre 0.0 y 1.0\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convertimos imágenes a Tensores\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizamos con media 0.5 y desviación estándar 0.5\n",
    "])\n",
    "\n",
    "# Cargamos el dataset MNIST\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Máximo valor de la imagen:\", train_data[0][0].squeeze().max())\n",
    "print(\"Mínimo valor de la imagen:\",train_data[0][0].squeeze().min())\n",
    "\n",
    "# DataLoader para los conjuntos de entrenamiento y prueba\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0\n",
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos a visualizar una imagen del dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Obtenemos una imagen del dataset\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "#Variables para la predicción\n",
    "single_label = labels[0]\n",
    "single_image = images[0]\n",
    "\n",
    "# Convertimos la imagen a numpy y \"desaplanamos\"/redimensionamos si es necesario\n",
    "# Además, deshacemos la normalización aplicada previamente\n",
    "image = images[0].numpy().squeeze()  # Eliminamos dimensiones de tamaño 1\n",
    "\n",
    "print(image.max(), image.min())\n",
    "image = (image * 0.5) + 0.5  # Deshacemos la normalización\n",
    "print(image.max(), image.min())\n",
    "\n",
    "# Visualizamos la imagen\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del batch de imágenes: torch.Size([64, 1, 28, 28])\n",
      "Tamaño del batch de etiquetas: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(\"Tamaño del batch de imágenes:\", images.shape)\n",
    "    print(\"Tamaño del batch de etiquetas:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTNet(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Número de parámetros: 535818\n"
     ]
    }
   ],
   "source": [
    "#Definimos el modelo\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        # Imágenes de 28x28 entradas a 512 neuronas [64, 784]\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        # 512 a 256 neuronas\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        # 256 a 10 neuronas (10 clases de MNIST)\n",
    "        self.fc3 = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplanamos la imagen\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instanciamos la red\n",
    "net = MNISTNet()\n",
    "\n",
    "print(net)\n",
    "print(\"Número de parámetros:\", sum(p.numel() for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta real: 9\n",
      "Predicción del modelo: 0\n"
     ]
    }
   ],
   "source": [
    "# Predicción antes del entrenamiento\n",
    "with torch.no_grad():\n",
    "    output = net(single_image.unsqueeze(0))  # Agregar dimensión batch\n",
    "    predicted_label = torch.argmax(output, dim=1)  # Índice de la mayor probabilidad\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Etiqueta real: {single_label.item()}\")\n",
    "print(f\"Predicción del modelo: {predicted_label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Pérdida: 0.9094348758109597\n",
      "Época 2, Pérdida: 0.3626765979489665\n",
      "Época 3, Pérdida: 0.30929436772140356\n",
      "Época 4, Pérdida: 0.27728572265425727\n",
      "Época 5, Pérdida: 0.251172219027779\n",
      "Época 6, Pérdida: 0.22787468654038048\n",
      "Época 7, Pérdida: 0.20713011514165128\n",
      "Época 8, Pérdida: 0.18927497975131088\n",
      "Época 9, Pérdida: 0.17379492379502573\n",
      "Época 10, Pérdida: 0.16031581167973627\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de la red\n",
    "for epoch in range(10):  # Número de épocas\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Limpiamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Calculamos las salidas\n",
    "        output = net(images)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(output, labels)\n",
    "        # Retropropagación\n",
    "        loss.backward()\n",
    "        # Actualizamos los pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Época {epoch + 1}, Pérdida: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo en el conjunto de prueba: 95.15%\n",
      "Etiqueta real: 9\n",
      "Predicción del modelo: 9\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        #predicción despues del entrenamiento\n",
    "        output = net(single_image.unsqueeze(0))  # Agregar dimensión batch\n",
    "        predicted_label = torch.argmax(output, dim=1)  # Índice de la mayor probabilidad\n",
    "\n",
    "print(f'Precisión del modelo en el conjunto de prueba: {100 * correct / total}%')\n",
    "print(f\"Etiqueta real: {single_label.item()}\")\n",
    "print(f\"Predicción del modelo: {predicted_label.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
